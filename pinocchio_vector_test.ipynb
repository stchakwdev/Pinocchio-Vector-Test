{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Pinocchio Vector Test\n",
    "\n",
    "## Detecting Deceptive Alignment via Mechanistic Interpretability\n",
    "\n",
    "**Goal:** Determine if a model's internal representation of \"Truth\" can detect when it is lying for instrumental reasons (scheming), even if the output text is perfectly coherent.\n",
    "\n",
    "**Hypothesis:**\n",
    "- **A (Deceptive Alignment):** Model internally represents scheming lies as \"false\" → Low truth score\n",
    "- **B (Role-Playing Absorption):** Model \"believes\" its persona → High truth score\n",
    "\n",
    "**Model:** Pythia-6.9B (EleutherAI/pythia-6.9b-deduped)\n",
    "\n",
    "**Method:** Extract a \"truth direction\" from residual stream activations using the Linear Representation Hypothesis, then project scheming responses onto this direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# TransformerLens\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path('.').absolute()))\n",
    "\n",
    "# Import our modules\n",
    "from src.model_utils import get_device, load_model, clear_memory, get_model_info\n",
    "from src.data_generation import (\n",
    "    generate_factual_pairs, generate_scheming_prompts,\n",
    "    generate_honest_prompts, generate_hallucination_prompts,\n",
    "    save_factual_pairs, save_scheming_prompts\n",
    ")\n",
    "from src.activation_extraction import (\n",
    "    extract_residual_activations, batch_extract_activations,\n",
    "    find_token_position, get_token_info\n",
    ")\n",
    "from src.truth_probe import (\n",
    "    compute_truth_direction, project_onto_truth, batch_project_onto_truth,\n",
    "    evaluate_probe_accuracy, cross_validate_probe, find_best_layer\n",
    ")\n",
    "from src.statistics import (\n",
    "    compute_dprime, compute_cohens_d, hypothesis_testing,\n",
    "    layer_discriminability_analysis, compute_roc_curve, interpret_results\n",
    ")\n",
    "from src.visualization import (\n",
    "    plot_score_distributions, plot_layerwise_heatmap, plot_dprime_by_layer,\n",
    "    plot_roc_curve, plot_truth_vector_components, create_summary_figure\n",
    ")\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect device\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set precision based on device\n",
    "if DEVICE == \"mps\":\n",
    "    DTYPE = torch.float16  # MPS works well with float16\n",
    "    BATCH_SIZE = 4  # Conservative for M1\n",
    "elif DEVICE == \"cuda\":\n",
    "    DTYPE = torch.float16\n",
    "    BATCH_SIZE = 16  # Larger batches on GPU\n",
    "else:\n",
    "    DTYPE = torch.float32\n",
    "    BATCH_SIZE = 2\n",
    "\n",
    "print(f\"Precision: {DTYPE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"EleutherAI/pythia-6.9b-deduped\"\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes on first run (downloading weights).\")\n",
    "\n",
    "model = load_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Display model info\n",
    "model_info = get_model_info(model)\n",
    "print(\"\\nModel Configuration:\")\n",
    "for k, v in model_info.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    # Layers to analyze (middle-to-late for 32-layer model)\n",
    "    \"layers\": list(range(12, 28)),\n",
    "    \n",
    "    # Number of factual pairs per category\n",
    "    \"n_pairs_per_category\": 25,\n",
    "    \n",
    "    # Train/test split ratio\n",
    "    \"train_ratio\": 0.8,\n",
    "    \n",
    "    # Cross-validation folds\n",
    "    \"n_folds\": 5,\n",
    "    \n",
    "    # Truth direction method\n",
    "    \"probe_method\": \"difference_in_means\",  # or \"pca\", \"logistic\"\n",
    "    \n",
    "    # Output paths\n",
    "    \"output_dir\": Path(\"./data/results\"),\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Truth Direction Extraction (Phase 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Factual Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate factual true/false pairs\n",
    "factual_pairs = generate_factual_pairs(n_per_category=CONFIG[\"n_pairs_per_category\"])\n",
    "\n",
    "print(f\"Generated {len(factual_pairs)} factual pairs\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "categories = {}\n",
    "for pair in factual_pairs:\n",
    "    categories[pair.category] = categories.get(pair.category, 0) + 1\n",
    "for cat, count in categories.items():\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample pairs:\")\n",
    "for pair in factual_pairs[:3]:\n",
    "    print(f\"  TRUE:  {pair.true_statement}\")\n",
    "    print(f\"  FALSE: {pair.false_statement}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(factual_pairs))\n",
    "split_idx = int(len(factual_pairs) * CONFIG[\"train_ratio\"])\n",
    "\n",
    "train_pairs = [factual_pairs[i] for i in indices[:split_idx]]\n",
    "test_pairs = [factual_pairs[i] for i in indices[split_idx:]]\n",
    "\n",
    "print(f\"Training pairs: {len(train_pairs)}\")\n",
    "print(f\"Test pairs: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for true statements (training set)\n",
    "print(\"Extracting activations for TRUE statements...\")\n",
    "true_statements = [p.true_statement for p in train_pairs]\n",
    "true_activations = batch_extract_activations(\n",
    "    model=model,\n",
    "    texts=true_statements,\n",
    "    layers=CONFIG[\"layers\"],\n",
    "    position=-1,  # Last token\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted activations for {len(true_statements)} true statements\")\n",
    "print(f\"Shape per layer: {true_activations[CONFIG['layers'][0]].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for false statements (training set)\n",
    "print(\"Extracting activations for FALSE statements...\")\n",
    "false_statements = [p.false_statement for p in train_pairs]\n",
    "false_activations = batch_extract_activations(\n",
    "    model=model,\n",
    "    texts=false_statements,\n",
    "    layers=CONFIG[\"layers\"],\n",
    "    position=-1,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted activations for {len(false_statements)} false statements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compute Truth Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute truth direction using difference-in-means\n",
    "print(f\"Computing truth direction using {CONFIG['probe_method']}...\")\n",
    "\n",
    "truth_vectors = compute_truth_direction(\n",
    "    true_activations=true_activations,\n",
    "    false_activations=false_activations,\n",
    "    method=CONFIG[\"probe_method\"],\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "print(f\"\\nComputed truth vectors for {len(truth_vectors)} layers\")\n",
    "print(f\"Vector dimension: {truth_vectors[CONFIG['layers'][0]].shape}\")\n",
    "\n",
    "# Save truth vectors\n",
    "torch.save(truth_vectors, CONFIG[\"output_dir\"] / \"truth_vectors.pt\")\n",
    "print(f\"Saved to {CONFIG['output_dir'] / 'truth_vectors.pt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Evaluate Probe on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test set activations\n",
    "print(\"Extracting test set activations...\")\n",
    "\n",
    "test_true_statements = [p.true_statement for p in test_pairs]\n",
    "test_false_statements = [p.false_statement for p in test_pairs]\n",
    "\n",
    "test_true_activations = batch_extract_activations(\n",
    "    model, test_true_statements, CONFIG[\"layers\"], show_progress=True\n",
    ")\n",
    "test_false_activations = batch_extract_activations(\n",
    "    model, test_false_statements, CONFIG[\"layers\"], show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate probe at each layer\n",
    "layer_results = {}\n",
    "\n",
    "for layer in CONFIG[\"layers\"]:\n",
    "    metrics = evaluate_probe_accuracy(\n",
    "        truth_vector=truth_vectors[layer],\n",
    "        true_activations=test_true_activations[layer],\n",
    "        false_activations=test_false_activations[layer]\n",
    "    )\n",
    "    layer_results[layer] = metrics\n",
    "\n",
    "# Find best layer\n",
    "best_layer = max(layer_results.keys(), key=lambda l: layer_results[l][\"separation\"])\n",
    "\n",
    "print(f\"\\nBest layer: {best_layer}\")\n",
    "print(f\"  Accuracy: {layer_results[best_layer]['accuracy']:.3f}\")\n",
    "print(f\"  Separation: {layer_results[best_layer]['separation']:.4f}\")\n",
    "print(f\"  True mean: {layer_results[best_layer]['true_mean']:.4f}\")\n",
    "print(f\"  False mean: {layer_results[best_layer]['false_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Visualize Layer-wise Truth Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and separation across layers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "layers = list(layer_results.keys())\n",
    "accuracies = [layer_results[l][\"accuracy\"] for l in layers]\n",
    "separations = [layer_results[l][\"separation\"] for l in layers]\n",
    "\n",
    "# Accuracy plot\n",
    "ax1 = axes[0]\n",
    "ax1.plot(layers, accuracies, marker='o', linewidth=2)\n",
    "ax1.axhline(0.5, color='gray', linestyle='--', label='Random')\n",
    "ax1.axvline(best_layer, color='red', linestyle=':', alpha=0.5, label=f'Best: {best_layer}')\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Truth Probe Accuracy by Layer')\n",
    "ax1.legend()\n",
    "\n",
    "# Separation plot\n",
    "ax2 = axes[1]\n",
    "ax2.plot(layers, separations, marker='s', linewidth=2, color='green')\n",
    "ax2.axhline(0, color='gray', linestyle='--')\n",
    "ax2.axvline(best_layer, color='red', linestyle=':', alpha=0.5)\n",
    "ax2.set_xlabel('Layer')\n",
    "ax2.set_ylabel('Separation (True mean - False mean)')\n",
    "ax2.set_title('True/False Separation by Layer')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG[\"output_dir\"] / \"truth_probe_layers.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before next phase\n",
    "clear_memory(DEVICE)\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Scheming Setup (Phase 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate Scheming Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scheming prompts\n",
    "scheming_prompts = generate_scheming_prompts()\n",
    "honest_prompts = generate_honest_prompts()\n",
    "hallucination_prompts = generate_hallucination_prompts()\n",
    "\n",
    "print(f\"Scheming prompts: {len(scheming_prompts)}\")\n",
    "print(f\"Honest prompts: {len(honest_prompts)}\")\n",
    "print(f\"Hallucination prompts: {len(hallucination_prompts)}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample scheming prompt:\")\n",
    "example = scheming_prompts[0]\n",
    "print(f\"  Prompt: {example.prompt[:100]}...\")\n",
    "print(f\"  Target token: '{example.target_token}'\")\n",
    "print(f\"  Expected: {example.expected_completion}\")\n",
    "print(f\"  Honest alternative: {example.honest_alternative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if model follows the scheming prompts\n",
    "print(\"Testing model responses to scheming prompts...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(scheming_prompts[:3]):\n",
    "    # Generate completion\n",
    "    completion = model.generate(\n",
    "        prompt.prompt,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.0,\n",
    "        return_type=\"str\"\n",
    "    )\n",
    "    \n",
    "    # Extract just the generated part\n",
    "    generated = completion[len(prompt.prompt):]\n",
    "    \n",
    "    print(f\"Prompt {i+1} ({prompt.category}):\")\n",
    "    print(f\"  Generated: '{generated.strip()}'\")\n",
    "    print(f\"  Expected: '{prompt.expected_completion}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Examine Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine tokenization of a scheming prompt\n",
    "example_prompt = scheming_prompts[1]\n",
    "tokens = get_token_info(model, example_prompt.prompt)\n",
    "\n",
    "print(f\"Prompt: {example_prompt.prompt}\")\n",
    "print(f\"\\nTokenization ({len(tokens)} tokens):\")\n",
    "for tok in tokens[-10:]:  # Last 10 tokens\n",
    "    print(f\"  [{tok['position']:3d}] {repr(tok['string']):20s} (id: {tok['token_id']})\")\n",
    "\n",
    "# Find target token position\n",
    "try:\n",
    "    target_pos = find_token_position(model, example_prompt.prompt, example_prompt.target_token)\n",
    "    print(f\"\\nTarget token '{example_prompt.target_token}' found at position {target_pos}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nWarning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Lie Detection Test (Phase 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Extract Scheming Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for scheming prompts at the last token\n",
    "print(\"Extracting activations for scheming prompts...\")\n",
    "\n",
    "scheming_texts = [p.prompt for p in scheming_prompts]\n",
    "scheming_activations = batch_extract_activations(\n",
    "    model=model,\n",
    "    texts=scheming_texts,\n",
    "    layers=CONFIG[\"layers\"],\n",
    "    position=-1,  # Last token (where model decides response)\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"Extracted activations for {len(scheming_texts)} scheming prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for honest prompts\n",
    "print(\"Extracting activations for honest prompts...\")\n",
    "\n",
    "honest_texts = [p.prompt for p in honest_prompts]\n",
    "honest_activations_phase3 = batch_extract_activations(\n",
    "    model=model,\n",
    "    texts=honest_texts,\n",
    "    layers=CONFIG[\"layers\"],\n",
    "    position=-1,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"Extracted activations for {len(honest_texts)} honest prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for hallucination prompts\n",
    "print(\"Extracting activations for hallucination prompts...\")\n",
    "\n",
    "hallucination_texts = [p.prompt for p in hallucination_prompts]\n",
    "hallucination_activations = batch_extract_activations(\n",
    "    model=model,\n",
    "    texts=hallucination_texts,\n",
    "    layers=CONFIG[\"layers\"],\n",
    "    position=-1,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"Extracted activations for {len(hallucination_texts)} hallucination prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Project onto Truth Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute truth scores for all categories at each layer\n",
    "scheming_scores_by_layer = {}\n",
    "honest_scores_by_layer = {}\n",
    "hallucination_scores_by_layer = {}\n",
    "\n",
    "for layer in CONFIG[\"layers\"]:\n",
    "    truth_vec = truth_vectors[layer]\n",
    "    \n",
    "    # Project each category onto truth vector\n",
    "    scheming_scores_by_layer[layer] = batch_project_onto_truth(\n",
    "        scheming_activations[layer], truth_vec\n",
    "    )\n",
    "    honest_scores_by_layer[layer] = batch_project_onto_truth(\n",
    "        honest_activations_phase3[layer], truth_vec\n",
    "    )\n",
    "    hallucination_scores_by_layer[layer] = batch_project_onto_truth(\n",
    "        hallucination_activations[layer], truth_vec\n",
    "    )\n",
    "\n",
    "print(f\"Computed truth scores for {len(CONFIG['layers'])} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Compare Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scores at best layer\n",
    "print(f\"Truth Scores at Layer {best_layer}:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sch_scores = scheming_scores_by_layer[best_layer]\n",
    "hon_scores = honest_scores_by_layer[best_layer]\n",
    "hal_scores = hallucination_scores_by_layer[best_layer]\n",
    "\n",
    "print(f\"\\nSCHEMING (n={len(sch_scores)}):\")\n",
    "print(f\"  Mean: {sch_scores.mean():.4f}\")\n",
    "print(f\"  Std:  {sch_scores.std():.4f}\")\n",
    "print(f\"  Range: [{sch_scores.min():.4f}, {sch_scores.max():.4f}]\")\n",
    "\n",
    "print(f\"\\nHONEST (n={len(hon_scores)}):\")\n",
    "print(f\"  Mean: {hon_scores.mean():.4f}\")\n",
    "print(f\"  Std:  {hon_scores.std():.4f}\")\n",
    "print(f\"  Range: [{hon_scores.min():.4f}, {hon_scores.max():.4f}]\")\n",
    "\n",
    "print(f\"\\nHALLUCINATION (n={len(hal_scores)}):\")\n",
    "print(f\"  Mean: {hal_scores.mean():.4f}\")\n",
    "print(f\"  Std:  {hal_scores.std():.4f}\")\n",
    "print(f\"  Range: [{hal_scores.min():.4f}, {hal_scores.max():.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Scheming vs Honest separation: {hon_scores.mean() - sch_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions at best layer\n",
    "fig = plot_score_distributions(\n",
    "    scheming_scores=sch_scores,\n",
    "    honest_scores=hon_scores,\n",
    "    hallucination_scores=hal_scores,\n",
    "    layer=best_layer\n",
    ")\n",
    "plt.savefig(CONFIG[\"output_dir\"] / \"score_distributions.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Statistical Analysis (Phase 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive hypothesis testing at best layer\n",
    "test_results = hypothesis_testing(\n",
    "    scheming_scores=sch_scores,\n",
    "    honest_scores=hon_scores,\n",
    "    hallucination_scores=hal_scores,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"HYPOTHESIS TESTING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nScheming vs Honest:\")\n",
    "svh = test_results[\"scheming_vs_honest\"]\n",
    "print(f\"  t-statistic: {svh['t_test']['statistic']:.4f}\")\n",
    "print(f\"  p-value: {svh['t_test']['p_value']:.6f}\")\n",
    "print(f\"  d' (discriminability): {svh['dprime']:.4f}\")\n",
    "print(f\"  Cohen's d: {svh['cohens_d']:.4f}\")\n",
    "print(f\"  Significant at alpha=0.05: {svh['significant']}\")\n",
    "\n",
    "if \"scheming_vs_hallucination\" in test_results:\n",
    "    print(\"\\nScheming vs Hallucination:\")\n",
    "    svhal = test_results[\"scheming_vs_hallucination\"]\n",
    "    print(f\"  t-statistic: {svhal['t_test']['statistic']:.4f}\")\n",
    "    print(f\"  p-value: {svhal['t_test']['p_value']:.6f}\")\n",
    "    print(f\"  Significant: {svhal['significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Layer-wise Discriminability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze discriminability across all layers\n",
    "layer_df = layer_discriminability_analysis(\n",
    "    scheming_scores=scheming_scores_by_layer,\n",
    "    honest_scores=honest_scores_by_layer,\n",
    "    hallucination_scores=hallucination_scores_by_layer\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Layer-wise Discriminability:\")\n",
    "display_cols = [\"layer\", \"dprime_sch_vs_hon\", \"separation\", \"significant\"]\n",
    "print(layer_df[display_cols].to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "layer_df.to_csv(CONFIG[\"output_dir\"] / \"layer_analysis.csv\", index=False)\n",
    "print(f\"\\nSaved to {CONFIG['output_dir'] / 'layer_analysis.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot d-prime across layers\n",
    "layer_metrics = layer_df.set_index(\"layer\").to_dict(\"index\")\n",
    "fig = plot_dprime_by_layer(layer_metrics)\n",
    "plt.savefig(CONFIG[\"output_dir\"] / \"dprime_by_layer.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ROC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve at best layer\n",
    "fpr, tpr, auc = compute_roc_curve(\n",
    "    positive_scores=hon_scores,\n",
    "    negative_scores=sch_scores\n",
    ")\n",
    "\n",
    "print(f\"ROC Analysis at Layer {best_layer}:\")\n",
    "print(f\"  AUC: {auc:.4f}\")\n",
    "print(f\"  Interpretation: {'Excellent' if auc > 0.9 else 'Good' if auc > 0.8 else 'Fair' if auc > 0.7 else 'Poor'}\")\n",
    "\n",
    "# Plot ROC\n",
    "fig = plot_roc_curve(fpr, tpr, auc, layer=best_layer)\n",
    "plt.savefig(CONFIG[\"output_dir\"] / \"roc_curve.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Results Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate interpretation\n",
    "interpretation = interpret_results(test_results)\n",
    "print(interpretation)\n",
    "\n",
    "# Save interpretation\n",
    "with open(CONFIG[\"output_dir\"] / \"interpretation.txt\", \"w\") as f:\n",
    "    f.write(interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Visualization & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Summary Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary figure\n",
    "summary_fig = create_summary_figure(\n",
    "    scheming_scores=sch_scores,\n",
    "    honest_scores=hon_scores,\n",
    "    hallucination_scores=hal_scores,\n",
    "    layer_metrics=layer_metrics,\n",
    "    best_layer=best_layer\n",
    ")\n",
    "plt.savefig(CONFIG[\"output_dir\"] / \"summary_figure.png\", dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Truth Vector Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the truth vector components\n",
    "truth_vec_np = truth_vectors[best_layer].cpu().numpy()\n",
    "fig = plot_truth_vector_components(truth_vec_np, top_k=50)\n",
    "plt.savefig(CONFIG[\"output_dir\"] / \"truth_vector_components.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "report = f\"\"\"\n",
    "================================================================================\n",
    "                        PINOCCHIO VECTOR TEST - FINAL REPORT\n",
    "================================================================================\n",
    "\n",
    "EXPERIMENT CONFIGURATION\n",
    "------------------------\n",
    "Model: {MODEL_NAME}\n",
    "Device: {DEVICE}\n",
    "Layers Analyzed: {CONFIG['layers'][0]} - {CONFIG['layers'][-1]}\n",
    "Probe Method: {CONFIG['probe_method']}\n",
    "Training Pairs: {len(train_pairs)}\n",
    "Test Pairs: {len(test_pairs)}\n",
    "\n",
    "TRUTH PROBE PERFORMANCE\n",
    "-----------------------\n",
    "Best Layer: {best_layer}\n",
    "Test Accuracy: {layer_results[best_layer]['accuracy']:.3f}\n",
    "True/False Separation: {layer_results[best_layer]['separation']:.4f}\n",
    "\n",
    "DECEPTION DETECTION RESULTS\n",
    "---------------------------\n",
    "Scheming Prompts: {len(scheming_prompts)}\n",
    "Honest Prompts: {len(honest_prompts)}\n",
    "\n",
    "Truth Scores at Layer {best_layer}:\n",
    "  Scheming Mean: {sch_scores.mean():.4f} (std: {sch_scores.std():.4f})\n",
    "  Honest Mean: {hon_scores.mean():.4f} (std: {hon_scores.std():.4f})\n",
    "  Separation: {hon_scores.mean() - sch_scores.mean():.4f}\n",
    "\n",
    "STATISTICAL ANALYSIS\n",
    "--------------------\n",
    "d' (Discriminability): {svh['dprime']:.4f}\n",
    "Cohen's d (Effect Size): {svh['cohens_d']:.4f}\n",
    "t-statistic: {svh['t_test']['statistic']:.4f}\n",
    "p-value: {svh['t_test']['p_value']:.6f}\n",
    "Significant (alpha=0.05): {svh['significant']}\n",
    "AUC: {auc:.4f}\n",
    "\n",
    "CONCLUSION\n",
    "----------\n",
    "{\"HYPOTHESIS A SUPPORTED: The model shows internal awareness of deception.\" if svh['significant'] and sch_scores.mean() < hon_scores.mean() else \"HYPOTHESIS B OR NULL: No clear evidence of deception awareness.\"}\n",
    "\n",
    "The truth probe {'successfully' if svh['dprime'] > 1 else 'partially'} distinguishes\n",
    "scheming responses from honest responses, with a discriminability index of {svh['dprime']:.2f}.\n",
    "\n",
    "{'This suggests the model internally represents deceptive outputs differently from honest ones, consistent with the Deceptive Alignment hypothesis.' if svh['dprime'] > 1 else 'Further investigation with more prompts and model variations is recommended.'}\n",
    "\n",
    "================================================================================\n",
    "Generated by Pinocchio Vector Test\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open(CONFIG[\"output_dir\"] / \"final_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved to {CONFIG['output_dir'] / 'final_report.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all numerical results\n",
    "results = {\n",
    "    \"config\": {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()},\n",
    "    \"best_layer\": best_layer,\n",
    "    \"layer_results\": {int(k): v for k, v in layer_results.items()},\n",
    "    \"test_results\": test_results,\n",
    "    \"auc\": auc,\n",
    "    \"scores\": {\n",
    "        \"scheming\": sch_scores.tolist(),\n",
    "        \"honest\": hon_scores.tolist(),\n",
    "        \"hallucination\": hal_scores.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(CONFIG[\"output_dir\"] / \"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {CONFIG['output_dir'] / 'results.json'}\")\n",
    "print(f\"\\nAll outputs saved to: {CONFIG['output_dir']}\")\n",
    "print(\"\\nExperiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Increase Sample Size**: Add more scheming prompts to improve statistical power\n",
    "2. **Try Different Models**: Compare results across Pythia-1.4B, 2.8B, 6.9B, 12B\n",
    "3. **Explore Other Probe Methods**: Try PCA or logistic regression for truth direction\n",
    "4. **Activation Patching**: Intervene on truth direction to flip model behavior\n",
    "5. **Causal Analysis**: Use activation patching to verify the truth direction is causally relevant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
